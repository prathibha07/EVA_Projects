Deep Learning for text and sequences 

Dataset used - IMDB movie-review dataset
Data preparation- Restricting the movie reviews to the top 10,000 most common words cut off the reviews after only 20 words. 
                  The network will learn 8-dimensional embeddings for each of the 10000 words, turn the input integer sequences (2D integer tensor) into embedding sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.

Some key observations : 
        GLOVE based word-embedding model with 8000 samples instead of 200 gave an accuracy of 81.5%
        Model without pretrained word embeddings gave an accuracy of 81.38%
        Accuracy might increase on model without pretrained word embeddings if tested on a different set of 10,000 samples.

Images path : EVA_Projects/phase2_s1/titlename 
Format used - title_name : observations

-glove_epochs : 
        Indicates the training of GLOVE based word-embedding model on 8000 samples and validation on 10000 samples for 10epochs.
        Accuracy achieved is 81.5%.
-glove_acc :
       Indicates plot of training, validation accuracy and loss. 
       It seems to be tring to do well.

-epochs_notpretrain : 
       Indicates the training of the same model without pretrained word embeddings on 8000 samples and validation on 10000 samples for 10epochs
       Accuracy achieved is 81.38% 
-valacc_notpretrain :
       Indicates the plot of training, validation accuracy and loss. 
       It seems to be overfitting. 
       
